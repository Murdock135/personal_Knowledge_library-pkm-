Notes from the book 'The definitive ANTLR guide' by 

---
# Section 1.2
```g4
//Hello.g4

grammar Hello; // Define a grammar called Hello
r : 'hello' ID ; // match keyword hello followed by an identifier
ID : [a-z]+ ; // match lower-case identifiers
WS : [ \t\r\n]+ -> skip ; // skip spaces, tabs, newlines, \r (Windows)
```
 We can generate the parser and lexer with
```sh
antlr4 Hello.g4
```
This will generate the following following files
```
Hello.tokens
HelloBaseListener.java
HelloLexer.java
HelloLexer.tokens
HelloListener.java
HelloParser.java
```
**The lexer will tokenize a given input stream** and the **parser documents the general structure of the language via a context free grammar**. 
We can now write a program that uses this lexer and parser to recognize code that is possibly written in this language.
# Tokens and actions
![[Pasted image 20251206222714.png]]In ANTLR, tokens can be 'sent' to either one of two channels: (i) Channel 0, called DEFAULT_CHANNEL, which the parser processes, and, (ii) Channel 1, called HIDDEN, which the parser ignores.
Tokens are produced by a **lexer**, which we are going to discuss next.
## Lexer
The lexer's job is to look at the '*input stream*' and categorize the stream of characters into tokens. It then sends the tokens into a *'token stream'*. It does this sequentially, looking at one character at a time. But how does the lexer do this? By following rules defined in a `.g4` file.
## Lexer actions
Lexer actions allow us to perform arbitrary *actions* upon matching a rule by using code (arbitrary code). This aforementioned code is written in the target language (the language that the parser is written in, generated by `antlr4 -Dlanguage=<language> <.g4>`). The action is executed before the token is *emitted* (sent to the token stream). 
## The `tokens { }` section
This is used to declare token types that have no lexer rules associated with them. In [[#Section 1.2]], rules `r, ID, WS` have rules associated with them (the CFG's after `:`). But we can create additional token types that have no lexer rules associated with them via
```
tokens {token_type_a, token_type_b} // token_type_a and token_type_b do not have lexer rules
```
But why use this? So that rules can be generated programmatically! (via actions).