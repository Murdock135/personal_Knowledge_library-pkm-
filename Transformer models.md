- Attention was used before in Neural Machine translation tasks. Transformers use attention to boost training speed. [1](https://jalammar.github.io/illustrated-transformer/)

# Catalogue
1. https://nlp.seas.harvard.edu/annotated-transformer/
2. 