- Attention was used before in Neural Machine translation tasks. Transformers use attention to boost training speed. [1](https://jalammar.github.io/illustrated-transformer/)
- The intuition behind attention is that rather than compressing the input, it might be better for the decoder to revisit the input sequence at every step [2](https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html)
- 

# Catalogue
1. https://nlp.seas.harvard.edu/annotated-transformer/
2. https://d2l.ai/chapter_attention-mechanisms-and-transformers/index.html